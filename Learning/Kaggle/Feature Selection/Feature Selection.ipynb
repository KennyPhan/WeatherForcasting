{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2b3627",
   "metadata": {},
   "source": [
    "Original Kaggle: https://www.kaggle.com/code/prashant111/comprehensive-guide-on-feature-selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefa7b2d",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d58382",
   "metadata": {},
   "source": [
    "Feature selection or variable selection is the process of selecting a subset of relevant features or variables from the total features of a level in a data set to build machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1530ed80",
   "metadata": {},
   "source": [
    "# Feature Selection – Techniques\n",
    "\n",
    "Feature selection techniques are categorized into 3 typers. These are as follows:\n",
    "\n",
    "1. Filter methods\n",
    "2. Wrapper methods\n",
    "3. Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc058fb",
   "metadata": {},
   "source": [
    "# 1. Filter Methods\n",
    "\n",
    "Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable.\n",
    "\n",
    "1. Basic methods\n",
    "2. Univariate methods\n",
    "3. Fischer score\n",
    "4. Information Gain\n",
    "5. ANOVA F-value\n",
    "6. Correlation Matrix with Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27706a",
   "metadata": {},
   "source": [
    "# 1.1 Basic Methods\n",
    "\n",
    "# 1.1.1 Removing Constant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041b24b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e83dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the customer dataset from Kaggle\n",
    "X_train = pd.read_csv('input/Customer/train.csv', nrows=35000)\n",
    "X_test = pd.read_csv('input/Customer/test.csv', nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000d5f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(labels=['TARGET'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e96615",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6286c888",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "sel = VarianceThreshold(threshold=0)\n",
    "sel.fit(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835d6d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_support is a boolean vector that indicates which features are retained\n",
    "# if we sum over get_support, we get the number of features that are not constant\n",
    "sum(sel.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a45c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constant features\n",
    "print(\n",
    "    len([\n",
    "        x for x in X_train.columns\n",
    "        if x not in X_train.columns[sel.get_support()]\n",
    "    ]))\n",
    "\n",
    "[x for x in X_train.columns if x not in X_train.columns[sel.get_support()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b438069",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e3474c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of training and test set\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d206cfa",
   "metadata": {},
   "source": [
    "# 1.1.2 Remove quasi-constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bb73e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('input/Customer/train.csv', nrows=35000)\n",
    "X_test = pd.read_csv('input/Customer/test.csv', nrows=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058bc2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(labels=['TARGET'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a007f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel = VarianceThreshold(threshold=0.01)  # 0.1 indicates 99% of observations approximately\n",
    "sel.fit(X_train)  # fit finds the features with low variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f264b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(sel.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1875fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quasi-constant features\n",
    "print(\n",
    "    len([\n",
    "        x for x in X_train.columns\n",
    "        if x not in X_train.columns[sel.get_support()]\n",
    "    ]))\n",
    "\n",
    "[x for x in X_train.columns if x not in X_train.columns[sel.get_support()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c49700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of observations showing each of the different values\n",
    "X_train['ind_var1'].value_counts() / np.float(len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sel.transform(X_train)\n",
    "X_test = sel.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2549d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f8cb62",
   "metadata": {},
   "source": [
    "# 1.2 Univariate selection methods \n",
    "Univariate feature selection methods works by selecting the best features based on univariate statistical tests like ANOVA. It can be seen as a preprocessing step to an estimator. Scikit-learn exposes feature selection routines as objects that implement the transform method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497507d4",
   "metadata": {},
   "source": [
    "# 1.2.1 SelectKBest\n",
    "\n",
    "This method select features according to the k highest scores. For instance, we can perform a chi-square test to the samples to retrieve only the two best features from iris dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1e05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be23e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the two best features\n",
    "X_new = SelectKBest(chi2, k=2).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d0b53",
   "metadata": {},
   "source": [
    "# 1.2.2 SelectPercentile\n",
    "Select features according to a percentile of the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e12bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectPercentile, chi2\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732f13b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectPercentile(chi2, percentile=10).fit_transform(X, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77955a6",
   "metadata": {},
   "source": [
    "# 1.3 Fisher Score (chi-square implementation) \n",
    "It is the chi-square implementation in scikit-learn. It computes chi-squared stats between each non-negative feature and class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5e679",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dd4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# load iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# create features and target\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# convert to categorical data by converting data to integers\n",
    "X = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37193967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Chi-Squared Statistics\n",
    "# select two features with highest chi-squared statistics\n",
    "chi2_selector = SelectKBest(chi2, k=2)\n",
    "X_kbest = chi2_selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef4a5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "print('Original number of features:', X.shape[1])\n",
    "print('Reduced number of features:', X_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e911b24",
   "metadata": {},
   "source": [
    "# 1.4 Information Gain\n",
    "Mutual information measures the information that X and Y share: It measures how much knowing one of these variables reduces uncertainty about the other. For example, if X and Y are independent, then knowing X does not give any information about Y and vice versa, so their mutual information is zero. At the other extreme, if X is a deterministic function of Y and Y is a deterministic function of X then all information conveyed by X is shared with Y: knowing X determines the value of Y and vice versa. As a result, in this case the mutual information is the same as the uncertainty contained in Y (or X) alone, namely the entropy of Y (or X). Moreover, this mutual information is the same as the entropy of X and as the entropy of Y. (A very special case of this is when X and Y are the same random variable.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360a3d6",
   "metadata": {},
   "source": [
    "# 1.5 ANOVA F-value\n",
    "Analysis of Variance (ANOVA) is a statistical formula used to compare variances across the means (or average) of different groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14918a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97aed26d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# Create features and target\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f031803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Features With Best ANOVA F-Values\n",
    "\n",
    "# Create an SelectKBest object to select features with two best ANOVA F-Values\n",
    "fvalue_selector = SelectKBest(f_classif, k=2)\n",
    "\n",
    "# Apply the SelectKBest object to the features and target\n",
    "X_kbest = fvalue_selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52087741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View results\n",
    "print('Original number of features:', X.shape[1])\n",
    "print('Reduced number of features:', X_kbest.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc1e13",
   "metadata": {},
   "source": [
    "# 1.6 Correlation-Matrix with Heatmap\n",
    "Correlation is a measure of the linear relationship of 2 or more variables. Through correlation, we can predict one variable from the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c5865e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load iris data\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Create features and target\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed42da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature matrix into DataFrame\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "# View the data frame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648483de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = df.corr()\n",
    "print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe26233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Correlation Heatmap of Iris Dataset')\n",
    "a = sns.heatmap(corr_matrix, square=True, annot=True, fmt='.2f', linecolor='black')\n",
    "a.set_xticklabels(a.get_xticklabels(), rotation=30)\n",
    "a.set_yticklabels(a.get_yticklabels(), rotation=30)           \n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac2e9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "upper   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5573d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find index of feature columns with correlation greater than 0.9\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]\n",
    "print(to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb51ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Marked Features\n",
    "df1 = df.drop(df.columns[to_drop], axis=1)\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b286aa",
   "metadata": {},
   "source": [
    "# Wrapper Methods\n",
    "In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from the subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "1. Forward Selection\n",
    "2. Backward Elimination\n",
    "3. Exhaustive Feature Selection\n",
    "4. Recursive Feature Elimination\n",
    "5. Recursive Feature Elimination with Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924cac18",
   "metadata": {},
   "source": [
    "# 2.1 Forward Selection\n",
    "Forward selection is an iterative method in which we start with having no feature in the model. In each iteration, we keep adding the feature which best improves our model till an addition of a new variable does not improve the performance of the model.\n",
    "\n",
    "The procedure starts with an empty set of features [reduced set]. The best of the original features is determined and added to the reduced set. At each subsequent iteration, the best of the remaining original attributes is added to the set.\n",
    "\n",
    "Step forward feature selection starts by evaluating all features individually and selects the one that generates the best performing algorithm, according to a pre-set evaluation criteria. In the second step, it evaluates all possible combinations of the selected feature and a second feature, and selects the pair that produce the best performing algorithm based on the same pre-set criteria.\n",
    "\n",
    "The pre-set criteria can be the roc_auc for classification and the r squared for regression for example.\n",
    "\n",
    "This selection procedure is called greedy, because it evaluates all possible single, double, triple and so on feature combinations. Therefore, it is quite computationally expensive, and sometimes, if feature space is big, even unfeasible.\n",
    "\n",
    "There is a special package for python that implements this type of feature selection: mlxtend.\n",
    "\n",
    "In the mlxtend implementation of the step forward feature selection, the stopping criteria is an arbitrarily set number of features. So the search will finish when we reach the desired number of selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32ce980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b530660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load dataset\n",
    "data = pd.read_csv('input/Home/train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96d8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4e1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find and remove correlated features\n",
    "def correlation(dataset, threshold):\n",
    "    col_corr = set()  # Set of all the names of correlated columns\n",
    "    corr_matrix = dataset.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n",
    "                colname = corr_matrix.columns[i]  # getting the name of column\n",
    "                col_corr.add(colname)\n",
    "    return col_corr\n",
    "\n",
    "corr_features = correlation(X_train, 0.8)\n",
    "print('correlated features: ', len(set(corr_features)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1327bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed correlated  features\n",
    "X_train.drop(labels=corr_features, axis=1, inplace=True)\n",
    "X_test.drop(labels=corr_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c5a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c654df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "sfs1 = SFS(RandomForestRegressor(), \n",
    "           k_features=10, \n",
    "           forward=True, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=3)\n",
    "\n",
    "sfs1 = sfs1.fit(np.array(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3429d2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.k_feature_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674574a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[list(sfs1.k_feature_idx_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1771e",
   "metadata": {},
   "source": [
    "We can see that forward feature selection results in the above columns being selected from all the given columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5431c508",
   "metadata": {},
   "source": [
    "# 2.2 Backward Elimination\n",
    "In backward elimination, we start with all the features and removes the least significant feature at each iteration which improves the performance of the model. We repeat this until no improvement is observed on removal of features.\n",
    "\n",
    "The procedure starts with the full set of attributes. At each step, it removes the worst attribute remaining in the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e260fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1 = SFS(RandomForestRegressor(), \n",
    "           k_features=10, \n",
    "           forward=False, \n",
    "           floating=False, \n",
    "           verbose=2,\n",
    "           scoring='r2',\n",
    "           cv=3)\n",
    "\n",
    "sfs1 = sfs1.fit(np.array(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e059ff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.k_feature_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf43cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.columns[list(sfs1.k_feature_idx_)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba5a73d",
   "metadata": {},
   "source": [
    "So, backward feature elimination results in the following columns being selected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40fb033",
   "metadata": {},
   "source": [
    "# 2.3 Exhaustive Feature Selection\n",
    "In an exhaustive feature selection the best subset of features is selected, over all possible feature subsets, by optimizing a specified performance metric for a certain machine learning algorithm.\n",
    "\n",
    "This is another greedy algorithm as it evaluates all possible feature combinations. It is quite computationally expensive, and sometimes, if feature space is big, even unfeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10bbd00",
   "metadata": {},
   "source": [
    "# 2.4 Recursive Feature Elimination\n",
    "It is a greedy optimization algorithm which aims to find the best performing feature subset. It repeatedly creates models and keeps aside the best or the worst performing feature at each iteration. It constructs the next model with the left features until all the features are exhausted. It then ranks the features based on the order of their elimination.\n",
    "\n",
    "Recursive feature elimination performs a greedy search to find the best performing feature subset. It iteratively creates models and determines the best or the worst performing feature at each iteration. It constructs the subsequent models with the left features until all the features are explored. It then ranks the features based on the order of their elimination. In the worst case, if a dataset contains N number of features RFE will do a greedy search for 2N combinations of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef186312",
   "metadata": {},
   "source": [
    "# 2.5 Recursive Feature Elimination with Cross-Validation\n",
    "Recursive Feature Elimination with Cross-Validated (RFECV) feature selection technique selects the best subset of features for the estimator by removing 0 to N features iteratively using recursive feature elimination.\n",
    "\n",
    "Then it selects the best subset based on the accuracy or cross-validation score or roc-auc of the model. Recursive feature elimination technique eliminates n features from a model by fitting the model multiple times and at each step, removing the weakest features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc838b6e",
   "metadata": {},
   "source": [
    "# Embedded Methods\n",
    "Embedded methods are iterative in a sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration. Regularization methods are the most commonly used embedded methods which penalize a feature given a coefficient threshold.\n",
    "\n",
    "This is why Regularization methods are also called penalization methods that introduce additional constraints into the optimization of a predictive algorithm (such as a regression algorithm) that bias the model toward lower complexity (fewer coefficients).\n",
    "\n",
    "1. LASSO\n",
    "2. Tree Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978add1b",
   "metadata": {},
   "source": [
    "# 3.1 LASSO Regression\n",
    "Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients.\n",
    "\n",
    "Regularisation consists in adding a penalty to the different parameters of the machine learning model to reduce the freedom of the model and in other words to avoid overfitting. In linear model regularisation, the penalty is applied over the coefficients that multiply each of the predictors. From the different types of regularisation, Lasso or l1 has the property that is able to shrink some of the coefficients to zero. Therefore, that feature can be removed from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ddf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f1651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('input/Home/train.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a626f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In practice, feature selection should be done after data pre-processing,\n",
    "# so ideally, all the categorical variables are encoded into numbers,\n",
    "# and then you can assess how deterministic they are of the target\n",
    "\n",
    "# here for simplicity I will use only numerical variables\n",
    "# select numerical columns:\n",
    "\n",
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cbcd3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.drop(labels=['SalePrice'], axis=1),\n",
    "    data['SalePrice'],\n",
    "    test_size=0.3,\n",
    "    random_state=0)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283e42b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the features in the house dataset are in very\n",
    "# different scales, so it helps the regression to scale them\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train.fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2f9142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Lasso Linear regression\n",
    "sel_ = SelectFromModel(Lasso(alpha=100))\n",
    "sel_.fit(scaler.transform(X_train.fillna(0)), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebd6e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_.get_support()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a list with the selected features and print the outputs\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "\n",
    "print('total features: {}'.format((X_train.shape[1])))\n",
    "print('selected features: {}'.format(len(selected_feat)))\n",
    "print('features with coefficients shrank to zero: {}'.format(\n",
    "    np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d2935f",
   "metadata": {},
   "source": [
    "We can see that Lasso regularisation helps to remove non-important features from the dataset. So, increasing the penalisation will result in increase the number of features removed. Therefore, we need to keep an eye and monitor that we don't set a penalty too high so that to remove even important features, or too low and then not remove non-important features.\n",
    "\n",
    "If the penalty is too high and important features are removed, we will notice a drop in the performance of the algorithm and then realise that we need to decrease the regularisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863e1737",
   "metadata": {},
   "source": [
    "# 3.2 Random Forest Importance\n",
    "Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0563262c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb332309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = pd.read_csv('input/mushrooms.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b38d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare feature vector and target variable\n",
    "X = df.drop(['class'], axis = 1)\n",
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef260e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "X = pd.get_dummies(X, prefix_sep='_')\n",
    "y = LabelEncoder().fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c770d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize feature vector\n",
    "X2 = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c5d073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X2, y, test_size = 0.30, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bdf65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the classifier with n_estimators = 100\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99520973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the classifier to the training set\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb40b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict on the test set\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1f23a",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "Decision Trees models which are based on ensembles (eg. Extra Trees and Random Forest) can be used to rank the importance of the different features. Knowing which features our model is giving most importance can be of vital importance to understand how our model is making it’s predictions (therefore making it more explainable). At the same time, we can get rid of the features which do not bring any benefit to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066a82f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize feature importance\n",
    "\n",
    "plt.figure(num=None, figsize=(10,8), dpi=80, facecolor='w', edgecolor='k')\n",
    "feat_importances = pd.Series(clf.feature_importances_, index= X.columns)\n",
    "feat_importances.nlargest(7).plot(kind='barh')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
